@inproceedings{Ishigami1991,
  title = {An Importance Quantification Technique in Uncertainty Analysis for Computer Models},
  booktitle = {[1990] {{Proceedings}}. {{First International Symposium}} on {{Uncertainty Modeling}} and {{Analysis}}},
  author = {Ishigami, T. and Homma, T.},
  year = {1991},
  pages = {398--403},
  publisher = {{IEEE Comput. Soc. Press}},
  location = {{College Park, MD, USA}},
  doi = {10.1109/ISUMA.1990.151285},
  urldate = {2022-08-08},
}


@Article{Marrel2009,
  author  = {Marrel, Amandine and Iooss, Bertrand and Laurent, Béatrice and Roustant, Olivier},
  journal = {Reliability Engineering \& System Safety},
  title   = {Calculations of {Sobol} indices for the {Gaussian} process metamodel},
  year    = {2009},
  number  = {3},
  pages   = {742--751},
  volume  = {94},
  doi     = {10.1016/j.ress.2008.07.008},
}


@Article{Sobol1999,
  author  = {Sobol', Ilya M. and Levitan, Yu L.},
  journal = {Computer Physics Communications},
  title   = {On the use of variance reducing multipliers in {Monte Carlo} computations of a global sensitivity index},
  year    = {1999},
  number  = {1},
  pages   = {52--61},
  volume  = {117},
  doi     = {10.1016/S0010-4655(98)00156-8},
}

@Article{Morris1993,
  author  = {Morris, Max D. and Mitchell, T. J. and Ylvisaker, D.},
  journal = {Technometrics},
  title   = {Bayesian design and analysis of computer experiments: Use of derivatives in surface prediction},
  year    = {1993},
  number  = {3},
  pages   = {245--255},
  volume  = {35},
  doi     = {10.1080/00401706.1993.10485320},
}

@TechReport{Harper1983,
  author      = {Harper, William V. and Gupta, Sumant K.},
  institution = {Office of Nuclear Waste Isolation, Battelle Memorial Institute},
  title       = {Sensitivity/uncertainty analysis of a borehole scenario comparing latin hypercube sampling and deterministic sensitivity approaches},
  year        = {1983},
  number      = {{BMI}/{ONWI}-516},
  location    = {Columbus, Ohio},
  url         = {https://inldigitallibrary.inl.gov/PRR/84393.pdf},
}

@Article{Becker2020,
  author   = {William Becker},
  journal  = {Reliability Engineering {\&} System Safety},
  title    = {Metafunctions for benchmarking in sensitivity analysis},
  year     = {2020},
  pages    = {107189},
  volume   = {204},
  abstract = {Comparison studies of global sensitivity analysis (GSA) approaches are limited in that they are performed on a single model or a small set of test functions, with a limited set of sample sizes and dimensionalities. This work introduces a ﬂexible ‘metafunction’ framework to benchmarking which randomly generates test problems of varying dimensionality and functional form using random combinations of plausible basis functions, and a range of sample sizes. The metafunction is tuned to mimic the characteristics of real models, in terms of the type of model response and the proportion of active model inputs. To demonstrate the framework, a comprehensive comparison of ten GSA approaches is performed in the screening setting, considering functions with up to 100 dimensions and up to 1000 model runs. The methods examined range from recent metamodelling approaches to elementary eﬀects and Monte Carlo estimators of the Sobol’ total eﬀect index. The results give a comparison in unprecedented depth, and show that on average and in the setting investigated, Monte Carlo estimators, particularly the VARS estimator, outperform metamodels. Indicatively, metamodels become competitive at around 10–20 runs per model input, but at lower ratios sampling-based approaches are more eﬀective as a screening tool.},
  doi      = {10.1016/j.ress.2020.107189},
}

@PhdThesis{Moon2010,
  author   = {Hyejung Moon},
  school   = {Ohio State University},
  title    = {Design and analysis of computer experiments for screening input variables},
  year     = {2010},
  address  = {Ohio},
  abstract = {A computer model is a computer code that implements a mathematical model of a physical process. A computer code is often complicated and can involve a large number of inputs, so it may take hours or days to produce a single response. Screening to determine the most active inputs is critical for reducing the number of future code runs required to understand the detailed input-output relationship, since the computer model is typically complex and the exact functional form of the input-output relationship is unknown. This dissertation proposes a new screening method that identifies active inputs in a computer experiment setting. It describes a Bayesian computation of sensitivity indices as screening measures. It provides algorithms for generating desirable designs for successful screening.

The proposed screening method is called GSinCE (Group Screening in Computer Experiments). The GSinCE procedure is based on a two-stage group screening approach, in which groups of inputs are investigated in the first stage and then inputs within only those groups identified as active at the first stage are investigated individually at the second stage. Two-stage designs with desirable properties are constructed to implement the procedure. Sensitivity indices are used to measure the effects of inputs on the response. Inputs with large sensitivity indices are determined by comparison with a benchmark null distribution constructed from user-specified, low-impact inputs. The use of low-impact inputs is useful for screening out inputs having small effects as well as those that are totally inert. Simulated examples show that, compared with one-stage procedures, the GSinCE procedure provides accurate screening while reducing computational effort.

In this dissertation, the sensitivity indices used as screening measures are computed in a Gaussian process model framework. This approach is known to be computationally efficient by using small numbers of expensive computer code runs for the estimation of sensitivity indices. The existing approach for quantitative inputs is extended so that sensitivity indices can be computed when inputs include a qualitative input in addition to quantitative inputs.

An orthogonal design in which the design matrix has uncorrelated columns is important for estimating the effects of inputs. Moreover, a space-filling design for which design points are well spread out is needed to explore the experimental region thoroughly. New algorithms for achieving such orthogonal space-filling designs are proposed in this dissertation. The three kinds of software are provided for the proposed GSinCE procedure, computation of sensitivity indices, and design search algorithms.},
  url      = {http://rave.ohiolink.edu/etdc/view?acc_num=osu1275422248},
}

@Book{Forrester2008,
  author     = {Forrester, Alexander I. J. and Sóbester, András and Keane, Andy J.},
  publisher  = {{Wiley}},
  title      = {Engineering {{Design}} via {{Surrogate Modelling}}: {{A Practical Guide}}},
  year       = {2008},
  edition    = {1},
  isbn       = {978-0-470-06068-1 978-0-470-77080-1},
  doi        = {10.1002/9780470770801},
  shorttitle = {Engineering {{Design}} via {{Surrogate Modelling}}},
}

@Article{Puy2022a,
  author  = {Arnald Puy and William Becker and Samuele Lo Piano and Andrea Saltelli},
  journal = {International Journal for Uncertainty Quantification},
  title   = {A comprehensive comparison of total-order estimators for global sensitivity analysis},
  year    = {2022},
  number  = {2},
  pages   = {1--18},
  volume  = {12},
  doi     = {10.1615/int.j.uncertaintyquantification.2021038133},
}

@Article{BenAri2007,
  author   = {Einat Neumann Ben-Ari and David M. Steinberg},
  journal  = {Quality Engineering},
  title    = {Modeling data from computer experiments: an empirical comparison of kriging with {MARS} and projection pursuit regression},
  year     = {2007},
  number   = {4},
  pages    = {327--338},
  volume   = {19},
  abstract = {Computer experiments enable scientists to study complex processes by running computer codes that simulate them. We consider here the analysis of data from computer experiments, comparing three methods for non-parametric smoothing of high-dimensional data: Kriging, Multivariate Adaptive Regression Splines (MARS) and Projection Pursuit Regression (PPR). On several data sets of varying complexity, we find that Kriging is consistently the best performer.},
  doi      = {10.1080/08982110701580930},
}

@TechReport{Worley1987,
  author      = {Brian A. Worley},
  institution = {Oak Ridge National Laboratory (ORNL)},
  title       = {Deterministic uncertainty analysis},
  year        = {1987},
  address     = {Oak Ridge, Tennessee},
  number      = {{ORNL}-6428},
  abstract    = {This paper presents a deterministic uncertainty analysis (DUA) method for calculating uncertainties that has the potential to significantly reduce the number of computer runs compared to conventional statistical analysis. The method is based upon the availability of derivative and sensitivity data such as that calculated using the well known direct or adjoint sensitivity analysis techniques. Formation of response surfaces using derivative data and the propagation of input probability distributions are discussed relative to their role in the DUA method. A sample problem that models the flow of water through a borehole is used as a basis to compare the cumulative distribution function of the flow rate as calculated by the standard statistical methods and the DUA method. Propogation of uncertainties by the DUA method is compared for ten cases in which the number of reference model runs was varied from one to ten. The DUA method gives a more accurate representation of the true cumulative distribution of the flow rate based upon as few as two model executions compared to fifty model executions using a statistical approach.},
  doi         = {10.2172/5534706},
}

@InProceedings{Zuhal2020,
  author    = {Lavi R. Zuhal and Kemas Zakaria and Pramudita S. Palar and Koji Shimoyama and Rhea P. Liem},
  booktitle = {{AIAA} Scitech 2020 Forum},
  title     = {Gradient-enhanced universal {Kriging} with polynomial chaos as trend function},
  year      = {2020},
  address   = {Orlando, Florida},
  publisher = {American Institute of Aeronautics and Astronautics},
  abstract  = {In this paper, we propose a new formulation of gradient-enhanced universal Kriging that uses sparse polynomial chaos expansion (PCE) as trend functions. In this regard, the gradient information is used to improve both the trend function and the Gaussian process part. The optimal set of polynomial terms is selected based on the least angle regression algorithm. We tested the performance of the posed gradient-enhanced polynomial chaos Kriging (GEPCK) in several algebraic and non-algebraic test cases and compared it with ordinary Kriging (OK) and ordinary gradient-enhanced Kriging (GEK). Results show that GEPCK consistently outperformed other methods or at least competitive to the best performing method on both algebraicand non-algebraic problems. This indicates that the performance of the conventional GEK can be further improved by incorporating sparse PCE as trend functions.},
  date      = {2020-01-06/2020-01-10},
  doi       = {10.2514/6.2020-1865},
}

@Article{Igusa1985,
  author    = {Takeru Igusa and Armen Der Kiureghian},
  journal   = {Journal of Engineering Mechanics},
  title     = {Dynamic characterization of two-degree-of-freedom equipment-structure systems},
  year      = {1985},
  number    = {1},
  pages     = {1--19},
  volume    = {111},
  abstract  = {A two‐degree‐of‐freedom equipment‐structure system is studied to find its intrinsic properties which are needed for analysis of more general secondary systems. Perturbation theory is used to find closed form expressions for the modal properties of the system in terms of the properties of the individual subsystems. Three important characteristics of the system are identified: tuning, interaction, and nonclassical damping. Mathematical expressions are defined for each of these characteristics and criteria are developed to measure their influences on the response of the equipment. The expressions for the modal properties and the criteria for tuning, interaction, and nonclassical damping are new results for the two‐degree‐of‐freedom system. These results form the bases for analysis of multiply supported multi‐degree‐of‐freedom secondary systems.},
  doi       = {10.1061/(asce)0733-9399(1985)111:1(1)},
  publisher = {American Society of Civil Engineers ({ASCE})},
}

@Article{DerKiureghian1991,
  author   = {Der Kiureghian, Armen and De Stefano, Mario},
  journal  = {Journal of Engineering Mechanics},
  title    = {Efficient algorithm for second-order reliability analysis},
  year     = {1991},
  number   = {12},
  pages    = {2904--2923},
  volume   = {117},
  abstract = {In the second‐order reliability method the principal curvatures of the limit‐state surface at the design point are used to construct a paraboloid approximation of the surface, which is then used to compute a second‐order estimate of the failure probability. The principal curvatures are the eigenvalues of the Hessian of the surface. In this paper an efficient algorithm is developed to determine the principal curvatures without computing the Hessian. The curvatures are computed in an iterative manner using the gradient of the limit‐state function, and are obtained in the decreasing order of their absolute magnitudes, which is also the order of their importance in reliability analysis. The computation can be terminated when the last curvature obtained is sufficiently small. The method is efficient for problems with large numbers of random variables, especially when an efficient algorithm for computing the gradient is available. Several numerical examples, including a finite‐element application involving 99 random variables, demonstrate the accuracy and efficiency of the method.},
  doi      = {10.1061/(asce)0733-9399(1991)117:12(2904)},
}

@PhdThesis{Dubourg2011,
  author   = {Dubourg, Vincent},
  school   = {Université Blaise Pascal - Clermont II},
  title    = {Adaptive surrogate models for reliability analysis and reliability-based design optimization},
  year     = {2011},
  address  = {Clermont-Ferrand, France},
  type     = {phdthesis},
  abstract = {This thesis is a contribution to the resolution of the reliability-based design optimization problem. This probabilistic design approach is aimed at considering the uncertainty attached to the system of interest in order to provide optimal and safe solutions. The safety level is quantified in the form of a probability of failure. Then, the optimization problem consists in ensuring that this failure probability remains less than a threshold specified by the stakeholders. The resolution of this problem requires a high number of calls to the limit-state design function underlying the reliability analysis. Hence it becomes cumbersome when the limit-state function involves an expensive-to-evaluate numerical model (e.g. a finite element model). In this context, this manuscript proposes a surrogate-based strategy where the limit-state function is progressively replaced by a Kriging meta-model. A special interest has been given to quantifying, reducing and eventually eliminating the error introduced by the use of this meta-model instead of the original model. The proposed methodology is applied to the design of geometrically imperfect shells prone to buckling.},
}

@Article{Sudret2008,
  author   = {Bruno Sudret},
  journal  = {Reliability Engineering {\&} System Safety},
  title    = {Global sensitivity analysis using polynomial chaos expansions},
  year     = {2008},
  number   = {7},
  pages    = {964--979},
  volume   = {93},
  abstract = {Global sensitivity analysis (SA) aims at quantifying the respective effects of input random variables (or combinations thereof) onto the variance of the response of a physical or mathematical model. Among the abundant literature on sensitivity measures, the Sobol’ indices have received much attention since they provide accurate information for most models. The paper introduces generalized polynomial chaos expansions (PCE) to build surrogate models that allow one to compute the Sobol’ indices analytically as a post-processing of the PCE coefﬁcients. Thus the computational cost of the sensitivity indices practically reduces to that of estimating the PCE coefﬁcients. An original non intrusive regression-based approach is proposed, together with an experimental design of minimal size. Various application examples illustrate the approach, both from the ﬁeld of global SA (i.e. well-known benchmark problems) and from the ﬁeld of stochastic mechanics. The proposed method gives accurate results for various examples that involve up to eight input random variables, at a computational cost which is 2–3 orders of magnitude smaller than the traditional Monte Carlo-based evaluation of the Sobol’ indices.},
  doi      = {10.1016/j.ress.2007.04.002},
}

@InCollection{Iooss2015,
  author    = {Bertrand Iooss and Paul Lemaître},
  booktitle = {Uncertainty Management in Simulation-Optimization of Complex Systems},
  publisher = {Springer {US}},
  title     = {A review on global sensitivity analysis methods},
  year      = {2015},
  pages     = {101--122},
  abstract  = {This chapter makes a review, in a complete methodological framework, of various global sensitivity analysis methods of model output. Numerous statistical and probabilistic tools (regression, smoothing, tests, statistical learning, Monte Carlo, …) aim at determining the model input variables which mostly contribute to an interest quantity depending on model output. This quantity can be for instance the variance of an output variable. Three kinds of methods are distinguished: the screening (coarse sorting of the most influential inputs among a large number), the measures of importance (quantitative sensitivity indices) and the deep exploration of the model behaviour (measuring the effects of inputs on their all variation range). A progressive application methodology is illustrated on a scholar application. A synthesis is given to place every method according to several axes, mainly the cost in number of model evaluations, the model complexity and the nature of brought information.},
  doi       = {10.1007/978-1-4899-7547-8_5},
}

@Article{Lamboni2013,
  author   = {M. Lamboni and B. Iooss and A.-L. Popelin and F. Gamboa},
  journal  = {Mathematics and Computers in Simulation},
  title    = {Derivative-based global sensitivity measures: {General} links with {Sobol'} indices and numerical tests},
  year     = {2013},
  pages    = {45--54},
  volume   = {87},
  abstract = {The estimation of variance-based importance measures (called Sobol’ indices) of the input variables of a numerical model can require a large number of model evaluations. It turns to be unacceptable for high-dimensional model involving a large number of input variables (typically more than ten). Recently, Sobol and Kucherenko have proposed the derivative-based global sensitivity measures (DGSM), defined as the integral of the squared derivatives of the model output, showing that it can help to solve the problem of dimensionality in some cases. We provide a general inequality link between DGSM and total Sobol’ indices for input variables belonging to the class of Boltzmann probability measures, thus extending the previous results of Sobol and Kucherenko for uniform and normal measures. The special case of log-concave measures is also described. This link provides a DGSM-based maximal bound for the total Sobol indices. Numerical tests show the performance of the bound and its usefulness in practice.},
  doi      = {10.1016/j.matcom.2013.02.002},
}

@InCollection{Baudin2017,
  author    = {Michaël Baudin and Anne Dutfoy and Bertrand Iooss and Anne-Laure Popelin},
  booktitle = {Handbook of Uncertainty Quantification},
  publisher = {Springer International Publishing},
  title     = {{OpenTURNS}: an industrial software for uncertainty quantification in simulation},
  year      = {2017},
  pages     = {2001--2038},
  abstract  = {The needs to assess robust performances for complex systems and to answer tighter regulatory processes (security, safety, environmental control, health impacts, etc.) have led to the emergence of a new industrial simulation challenge: to take uncertainties into account when dealing with complex numerical simulation frameworks. Therefore, a generic methodology has emerged from the joint effort of several industrial companies and academic institutions. EDF R&D, Airbus Group, and Phimeca Engineering started a collaboration at the beginning of 2005, joined by IMACS in 2014, for the development of an open-source software platform dedicated to uncertainty propagation by probabilistic methods, named OpenTURNS for open-source treatment of uncertainty, Risk ’N Statistics. OpenTURNS addresses the specific industrial challenges attached to uncertainties, which are transparency , genericity , modularity, and multi-accessibility. This paper focuses on OpenTURNS and presents its main features: OpenTURNS is an open- source software under the LGPL license that presents itself as a C++ library and a Python TUI and which works under Linux and Windows environment. All the methodological tools are described in the different sections of this paper: uncertainty quantification, uncertainty propagation, sensitivity analysis, and metamodeling. A section also explains the generic wrappers’ way to link OpenTURNS to any external code. The paper illustrates as much as possible the methodological tools on an educational example that simulates the height of a river and compares it to the height of a dike that protects industrial facilities. At last, it gives an overview of the main developments planned for the next few years.},
  doi       = {10.1007/978-3-319-12385-1_64},
}

@Article{Sobol1998,
  author   = {Sobol', Ilya M.},
  journal  = {Mathematics and Computers in Simulation},
  title    = {On quasi-{Monte Carlo} integrations},
  year     = {1998},
  number   = {2-5},
  pages    = {103--112},
  volume   = {47},
  abstract = {Relations between Monte Carlo and quasi-Monte Carlo methods are analysed from both theoretical and practical points of view with special emphasis on high-dimensional integration.},
  doi      = {10.1016/s0378-4754(98)00096-2},
}

@Article{Kucherenko2011,
  author   = {Sergei Kucherenko and Balazs Feil and Nilay Shah and Wolfgang Mauntz},
  journal  = {Reliability Engineering {\&} System Safety},
  title    = {The identification of model effective dimensions using global sensitivity analysis},
  year     = {2011},
  number   = {4},
  pages    = {440--449},
  volume   = {96},
  abstract = {It is shown that the effective dimensions can be estimated at reasonable computational costs using variance based global sensitivity analysis. Namely, the effective dimension in the truncation sense can be found by using the Sobol' sensitivity indices for subsets of variables. The effective dimension in the superposition sense can be estimated by using the first order effects and the total Sobol' sensitivity indices. The classification of some important classes of integrable functions based on their effective dimension is proposed. It is shown that it can be used for the prediction of the QMC efficiency. Results of numerical tests verify the prediction of the developed techniques.},
  doi      = {10.1016/j.ress.2010.11.003},
}

@Article{Marrel2008,
  author   = {Marrel, Amandine and Iooss, Bertrand and Van Dorpe, François and Volkova, Elena},
  journal  = {Computational Statistics {\&} Data Analysis},
  title    = {An efficient methodology for modeling complex computer codes with{ Gaussian} processes},
  year     = {2008},
  number   = {10},
  pages    = {4731--4744},
  volume   = {52},
  abstract = {Complex computer codes are often too time expensive to be directly used to perform uncertainty propagation studies, global sensitivity analysis or to solve optimization problems. A well known and widely used method to circumvent this inconvenience consists in replacing the complex computer code by a reduced model, called a metamodel, or a response surface that represents the computer code and requires acceptable calculation time. One particular class of metamodels is studied: the Gaussian process model that is characterized by its mean and covariance functions. A specific estimation procedure is developed to adjust a Gaussian process model in complex cases (non-linear relations, highly dispersed or discontinuous output, high-dimensional input, inadequate sampling designs, etc.). The efficiency of this algorithm is compared to the efficiency of other existing algorithms on an analytical test case. The proposed methodology is also illustrated for the case of a complex hydrogeological computer code, simulating radionuclide transport in groundwater.},
  doi      = {10.1016/j.csda.2008.03.026},
}

@Unpublished{Crestaux2007,
  author = {Crestaux, Thierry and Martinez, Jean-Marc and Le Maître, Olivier and Lafitte, Olivier},
  note   = {Presented at the Fifth International Conference on Sensitivity Analysis of Model Output},
  title  = {Polynomial chaos expansion for uncertainties quantification and sensitivity analysis},
  year   = {2007},
  url    = {http://samo2007.chem.elte.hu/lectures/Crestaux.pdf},
}

@Article{Radovic1996,
  author  = {Radović, Igor and Sobol', Ilya M. and Tichy, Robert F.},
  journal = {Monte Carlo Methods and Applications},
  title   = {{Quasi-Monte Carlo} methods for numerical integration: comparison of different low discrepancy sequences},
  year    = {1996},
  number  = {1},
  pages   = {1--14},
  volume  = {2},
  doi     = {10.1515/mcma.1996.2.1.1},
}

@Article{Charlson1992,
  author  = {R. J. Charlson and S. E. Schwartz and J. M. Hales and R. D. Cess and J. A. Coakley and J. E. Hansen and D. J. Hofmann},
  journal = {Science},
  title   = {Climate forcing by anthropogenic aerosols},
  year    = {1992},
  number  = {5043},
  pages   = {423--430},
  volume  = {255},
  doi     = {10.1126/science.255.5043.423},
}

@Article{Kopp2011,
  author    = {Greg Kopp and Judith L. Lean},
  journal   = {Geophysical Research Letters},
  title     = {A new, lower value of total solar irradiance: Evidence and climate significance},
  year      = {2011},
  month     = {jan},
  number    = {1},
  pages     = {n/a--n/a},
  volume    = {38},
  doi       = {10.1029/2010gl045777},
  publisher = {American Geophysical Union ({AGU})},
}

@Article{Penner1994,
  author  = {J. E. Penner and R. J. Charlson and S. E. Schwartz and J. M. Hales and N. S. Laulainen and L. Travis and R. Leifer and T. Novakov and J. Ogren and L. F. Radke},
  journal = {Bulletin of the American Meteorological Society},
  title   = {Quantifying and minimizing uncertainty of climate forcing by anthropogenic aerosols},
  year    = {1994},
  number  = {3},
  pages   = {375--400},
  volume  = {75},
  doi     = {10.1175/1520-0477(1994)075<0375:qamuoc>2.0.co;2},
}

@Article{Tatang1997,
  author  = {Menner A. Tatang and Wenwei Pan and Ronald G. Prinn and Gregory J. McRae},
  journal = {Journal of Geophysical Research: Atmospheres},
  title   = {An efficient method for parametric uncertainty analysis of numerical geophysical models},
  year    = {1997},
  number  = {D18},
  pages   = {21925--21932},
  volume  = {102},
  doi     = {10.1029/97jd01654},
}

@InBook{Pidwirny2006,
  author    = {Pidwirny, M.},
  chapter   = {Introduction to the oceans},
  publisher = {Michael Pidwirny},
  title     = {{Fundamentals of Physical Geography, 2nd Edition}},
  year      = {2006},
  url       = {http://www.physicalgeography.net/fundamentals/8o.html},
}

@Comment{jabref-meta: databaseType:bibtex;}
